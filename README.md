# transformers-mathematics

GPT-2 sucks at third grade math, I wonder if we can do better

We will be using [this dataset](https://github.com/deepmind/mathematics_dataset).
 
Joint work by [Helen Ngo](https://github.com/mathemakitten), [Joseph Palermo](https://github.com/joepalermo) and [Michael Jia](https://github.com/michaelzcjia).

### Benchmarks TODO
All character-level.
* [ ] LSTM with teacher forcing
* [ ] tiny Transformer 
    * [ ] add Encoder 
* [ ] regular Transformer 
* [ ] 1558M GPT-2 finetune, but there's some nuance here
