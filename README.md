# transformers-mathematics

GPT-2 sucks at third grade math, I wonder if we can do better

We will be using [this dataset](https://github.com/deepmind/mathematics_dataset).

### Benchmarks TODO
All character-level.
* [ ] LSTM with teacher forcing
* [ ] tiny Transformer 
    * [ ] add Encoder 
* [ ] regular Transformer 
* [ ] 1558M GPT-2 finetune, but there's some nuance here