# transformers-mathematics

GPT-2 sucks at third grade math, I wonder if we can do better

We will be using [this dataset](https://github.com/deepmind/mathematics_dataset).
 
Joint work by [Helen Ngo](https://github.com/mathemakitten), [Joseph Palermo](https://github.com/joepalermo) and [Michael Jia](https://github.com/michaelzcjia), 
with support from [Rayhane Mama](https://github.com/Rayhane-mamah).

### Benchmarks TODO
All character-level.
* [ ] LSTM with teacher forcing
* [ ] tiny Transformer 
    * [ ] add Encoder 
* [ ] regular Transformer 
* [ ] 1558M GPT-2 finetune, but there's some nuance here

### Other resources for mathematical reasoning
This is mostly a to-read list.

* [SemEval 2019: Math Question Answering](https://github.com/allenai/semeval-2019-task-10)
* [CodaLab Competition on Math QA](https://competitions.codalab.org/competitions/20013)
* [this paper on symbolic integration and solving diffEQ](https://arxiv.org/abs/1912.01412)